{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Stable Video Diffusion Img2Vid XT (Research Purpose)\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_video_diffusion_img2vid_xt.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_video_diffusion_img2vid_xt.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_stable_video_diffusion_img2vid_xt.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "Open in Vertex AI Workbench\n",
    "    </a>\n",
    "    (a Python-3 GPU notebook with preinstalled HuggingFace/transformer libraries is recommended)\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates running local inference for [stabilityai/stable-video-diffusion-img2vid-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt) on either [Colab](https://colab.research.google.com) or [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench). This notebook also demonstrates deploying it on Vertex AI for batch prediction for research purpose only.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Run local predictions for image-to-video.\n",
    "- Upload the model and run batch predictions for image-to-video.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "**NOTE**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioensNKM8ned"
   },
   "source": [
    "### Setup notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d73ffa0c0b83"
   },
   "source": [
    "#### Colab only\n",
    "Run the following commands for Colab and skip this section if you are using Workbench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2707b02ef5df"
   },
   "outputs": [],
   "source": [
    "# if \"google.colab\" in str(get_ipython()):\n",
    "#     ! pip3 install --upgrade google-cloud-aiplatform\n",
    "#     from google.colab import auth as google_auth\n",
    "\n",
    "#     google_auth.authenticate_user()\n",
    "\n",
    "# ! pip3 install --upgrade pip\n",
    "# ! pip3 install torch==2.0.1+cu118  -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# ! pip3 install transformers==4.36.2\n",
    "# ! pip3 install diffusers==0.25.0\n",
    "# ! pip3 install datasets==2.9.0\n",
    "# ! pip3 install accelerate==0.25.0\n",
    "\n",
    "# # Install gdown for downloading example training images.\n",
    "# ! pip3 install gdown\n",
    "# # Remove wrong cublas version.\n",
    "# ! pip3 uninstall nvidia_cublas_cu11 --yes\n",
    "\n",
    "# # Restart the notebook kernel after installs.\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb671e75ca7b"
   },
   "source": [
    "#### Workbench only\n",
    "1.   Follow [this link](https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_stable_video_diffusion_img2vid_xt.ipynb) to deploy the notebook to a Vertex AI Workbench Instance.\n",
    "2.   Select `Create a new Notebook`.\n",
    "3.   Click `Advanced Options`.\n",
    "4.   In the **Environment** tab, select `Debian 10` for **Operating System** and select `Custom Container` for **Environment**.\n",
    "5.   Set `Docker container image` to `us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/transformers-notebook`.\n",
    "6.   Under **Machine configuration**, select 1 `T4` GPU and select `Install NVIDIA GPU driver automatically for me`.\n",
    "7.   Click `Create` to create the Vertex AI Workbench instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7adab99e41"
   },
   "source": [
    "### Setup Google Cloud project\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
    "\n",
    "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
    "\n",
    "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c460088b873"
   },
   "source": [
    "Fill following variables for experiments environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Creating gs://injae-sandbox-340804-flux-us-central1/...\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.storage.buckets.create) HTTPError 409: Your previous request to create the named bucket succeeded and you already own it.\n",
      "mkdir: cannot create directory ‘content’: File exists\n"
     ]
    }
   ],
   "source": [
    "# # Cloud project id.\n",
    "# PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# # The region you want to launch jobs in.\n",
    "# REGION = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# # The Cloud Storage bucket for storing experiments output. Fill it without the 'gs://' prefix.\n",
    "# GCS_BUCKET = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# The service account for deploying fine tuned model.\n",
    "# SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "\n",
    "import socket\n",
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "SERVICE_ACCOUNT = SVC_ACC\n",
    "\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "LOCATION=\"us-central1\"\n",
    "\n",
    "UNIQUE_PREFIX = socket.gethostname()\n",
    "UNIQUE_PREFIX = re.sub('[^A-Za-z0-9]+', '', UNIQUE_PREFIX)\n",
    "\n",
    "UID = UNIQUE_PREFIX\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-{UNIQUE_PREFIX}-{LOCATION}\"\n",
    "\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud storage buckets create {BUCKET_URI} --project={PROJECT_ID} --location={LOCATION}\n",
    "\n",
    "!mkdir content\n",
    "!rm -r content/clips/\n",
    "!mkdir content/clips/\n",
    "!rm -r content/frames/\n",
    "!mkdir content/frames/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e828eb320337"
   },
   "source": [
    "Initialize Vertex-AI API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "12cd25839741"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cc825514deb"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b42bd4fa2b2d"
   },
   "outputs": [],
   "source": [
    "# The pre-built serving docker image. It contains serving scripts and models.\n",
    "\n",
    "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240605_1400_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "import imageio\n",
    "from diffusers.utils import load_image\n",
    "from google.cloud import aiplatform, storage\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def load_and_resize_image(image_url):\n",
    "    image = load_image(image_url)\n",
    "    # required by the model\n",
    "    new_width = round(image.size[0] / 8) * 8\n",
    "    new_height = round(image.size[1] / 8) * 8\n",
    "    return image.resize((new_width, new_height))\n",
    "\n",
    "\n",
    "def create_job_name(prefix):\n",
    "    user = os.environ.get(\"USER\")\n",
    "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    job_name = f\"{prefix}-{user}-{now}\"\n",
    "    return job_name\n",
    "\n",
    "\n",
    "def image_to_base64(image, format=\"JPEG\"):\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=format)\n",
    "    image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return image_str\n",
    "\n",
    "\n",
    "def display_video_frames(frames, fps=7, width=500):\n",
    "    io_obj = io.BytesIO()\n",
    "    imageio.mimsave(io_obj, frames, format=\"mp4\")\n",
    "    video_data = base64.b64encode(io_obj.getvalue()).decode(\"utf-8\")\n",
    "    html = \"\"\n",
    "    html += f\"<video controls width={width}>\"\n",
    "    html += f'<source src=\"data:video/mp4;base64,{video_data}\" type=\"video/mp4\">'\n",
    "    html += \"</video>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def upload_model(model_id, task):\n",
    "    model_name = model_id\n",
    "    serving_env = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"TASK\": task,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_bucket_and_blob_name(filepath):\n",
    "    # The gcs path is of the form gs://<bucket-name>/<blob-name>\n",
    "    gs_suffix = filepath.split(\"gs://\", 1)[1]\n",
    "    return tuple(gs_suffix.split(\"/\", 1))\n",
    "\n",
    "\n",
    "def upload_local_dir_to_gcs(local_dir_path, gcs_dir_path):\n",
    "    \"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    for local_file in glob.glob(local_dir_path + \"/**\"):\n",
    "        if not os.path.isfile(local_file):\n",
    "            continue\n",
    "        filename = local_file[1 + len(local_dir_path) :]\n",
    "        gcs_file_path = os.path.join(gcs_dir_path, filename)\n",
    "        _, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.upload_from_filename(local_file)\n",
    "        print(\"Copied {} to {}.\".format(local_file, gcs_file_path))\n",
    "\n",
    "\n",
    "def download_gcs_blob_as_json(gcs_file_path):\n",
    "    \"\"\"Download GCS blob and convert it to json.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket_name, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    return json.loads(blob.download_as_bytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqRm3WhGDo94"
   },
   "source": [
    "## Image to Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "oDe1WDBsTGuH"
   },
   "outputs": [],
   "source": [
    "# @title Define model and task\n",
    "model_id = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
    "task = \"image-to-video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "image = load_and_resize_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\"\n",
    ")\n",
    "\n",
    "# Create a list of frames by repeating the same image\n",
    "frames = [image] * 5  # Repeat the image 5 times\n",
    "\n",
    "# Save the frames as an MP4 video\n",
    "imageio.mimsave(\"rocket_video.mp4\", frames, format=\"mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "JrNS02rEDyKg"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd056d9c308402cbc3068ed7e87ebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function PluginV3.__del__ at 0x7f97f703ab90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/imageio/core/v3_plugin_api.py\", line 370, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/imageio/plugins/pillow.py\", line 144, in close\n",
      "    self._flush_writer()\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/imageio/plugins/pillow.py\", line 485, in _flush_writer\n",
      "    primary_image.save(self._request.get_file(), **self.save_args)\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/PIL/Image.py\", line 2546, in save\n",
      "    raise ValueError(msg) from e\n",
      "ValueError: unknown file extension: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d05225f88c41518d16421f3c554fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Run inferences locally\n",
    "import torch\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "\n",
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, variant=\"fp16\"\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# Load the conditioning image\n",
    "image = load_and_resize_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png\"\n",
    ")\n",
    "\n",
    "generator = torch.manual_seed(42)\n",
    "frames_list = pipe([image], decode_chunk_size=8, generator=generator).frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>,\n",
       "  <PIL.Image.Image image mode=RGB size=1024x576>]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frames_list\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Assuming frames_list is a list containing a single list of PIL Images\n",
    "frames = frames_list[0] \n",
    "\n",
    "for frame in frames:\n",
    "  display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video_frames(frames_list[0], fps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"rocket_video.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from IPython.display import Video\n",
    "\n",
    "# # Replace 'your_video.mp4' with the actual path to your video file\n",
    "# video_path = 'rocket_video.mp4' \n",
    "\n",
    "# # Embed the video within the notebook\n",
    "# Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG3p4-2eD0nQ"
   },
   "source": [
    "### Deploy model to run prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXPgPmkzD3f7"
   },
   "source": [
    "Deploy the stable video diffusion model for the `image-to-video` task.\n",
    "\n",
    "Once deployed, you can run batch prediction with a batch of images to generate videos.\n",
    "\n",
    "When deployed on one `NVIDIA_L4` GPU, the averaged inference time of a request is ~600 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz0vrpddEIj_"
   },
   "outputs": [],
   "source": [
    "# @title Define upload model\n",
    "\n",
    "model = upload_model(model_id=model_id, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCEX767pET2t"
   },
   "outputs": [],
   "source": [
    "# @title Prepare input\n",
    "import json\n",
    "\n",
    "image = load_and_resize_image(\n",
    "    \"https://images.pexels.com/videos/853848/free-video-853848.jpg\"\n",
    ")\n",
    "display(image)\n",
    "\n",
    "instances = [\n",
    "    {\"prompt\": \"placehoder\", \"image\": image_to_base64(image), \"motion_bucket_id\": 127}\n",
    "]\n",
    "\n",
    "# Convert and write JSON object to file\n",
    "os.makedirs(\"bath_prediction_input\", exist_ok=True)\n",
    "with open(\"bath_prediction_input/input.jsonl\", \"w\") as outfile:\n",
    "    for item in instances:\n",
    "        json_str = json.dumps(item)\n",
    "        outfile.write(json_str)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "upload_local_dir_to_gcs(\n",
    "    \"bath_prediction_input\", f\"gs://{BUCKET_URI}/image_to_video/input\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzW5MsK7EEym"
   },
   "source": [
    "Upload model. For batch predictions, only model upload is needed. you don't need to deploy the model to any endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLnf_TukFuKc"
   },
   "outputs": [],
   "source": [
    "# @title Run batch prediction\n",
    "\n",
    "machine_type = \"n1-standard-32\"\n",
    "accelerator_type = \"NVIDIA_TESLA_P100\"\n",
    "accelerator_count = 2\n",
    "\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    instances_format=\"jsonl\",\n",
    "    job_display_name=\"batch_predict_image_to_video\",\n",
    "    gcs_source=[f\"gs://{BUCKET_URI}/image_to_video/input/input.jsonl\"],\n",
    "    gcs_destination_prefix=f\"gs://{BUCKET_URI}/image_to_video/output\",\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    starting_replica_count=1,\n",
    ")\n",
    "\n",
    "batch_prediction_job.wait()\n",
    "\n",
    "print(batch_prediction_job.display_name)\n",
    "print(batch_prediction_job.resource_name)\n",
    "print(batch_prediction_job.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTp_AMSUF4k3"
   },
   "outputs": [],
   "source": [
    "# @title Display results\n",
    "\n",
    "gcs_output_file = (\n",
    "    batch_prediction_job.output_info.gcs_output_directory\n",
    "    + \"/prediction.results-00000-of-00001\"\n",
    ")\n",
    "prediction_result = download_gcs_blob_as_json(gcs_output_file)\n",
    "\n",
    "\n",
    "html = \"\"\n",
    "html += \"<video controls width=500>\"\n",
    "html += f'<source src=\"data:video/mp4;base64,{prediction_result[\"prediction\"]}\" type=\"video/mp4\">'\n",
    "html += \"</video>\"\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b53b883257b4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Delete model.\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model_garden_pytorch_stable_video_diffusion_img2vid_xt.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
